# Ollama Configuration
# --------------------
# Base URL for the Ollama API server
OLLAMA_BASE_URL=http://localhost:11434

# Default LLM model to use when starting the chat
# Make sure this model is installed (ollama pull <model-name>)
OLLAMA_DEFAULT_MODEL=llama3

# Command to start Ollama server if it's not already running
# Uncomment and set this to enable automatic Ollama startup
# Examples:
#   - Direct: OLLAMA_START_COMMAND="ollama serve"
#   - macOS Homebrew: OLLAMA_START_COMMAND="brew services start ollama"
#   - Linux systemd: OLLAMA_START_COMMAND="systemctl start ollama"
# OLLAMA_START_COMMAND="ollama serve"

# Phoenix Server Configuration
# -----------------------------
# Port to run the Phoenix web server on
PORT=4000

# Production Configuration (optional)
# -----------------------------------
# Only needed when running in production mode

# Generate a secret key with: mix phx.gen.secret
# SECRET_KEY_BASE=your_secret_key_here

# Your domain name (for production deployments)
# PHX_HOST=example.com

# Enable the Phoenix server (for releases)
# PHX_SERVER=true
